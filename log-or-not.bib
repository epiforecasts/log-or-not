
@article{bracherEvaluatingEpidemicForecasts2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {PLoS computational biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  langid = {english},
  pmcid = {PMC7880475},
  pmid = {33577550},
  keywords = {Communicable Diseases,COVID-19,Forecasting,Humans,Pandemics,Probability,SARS-CoV-2},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EX37R6J8/Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf}
}

@misc{bracherNationalSubnationalShortterm2021,
  title = {National and Subnational Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}}, Early 2021},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, J. and G{\"o}rgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, N. I. and Burgard, J. P. and Fiedler, J. and Fuhrmann, J. and Funk, S. and Gambin, A. and Gogolewski, K. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Leith{\"a}user, N. and Li, M. L. and Meinke, J. H. and Miasojedow, B. and Mohring, J. and Nouvellet, P. and Nowosielski, J. M. and Ozanski, T. and Radwan, M. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Gneiting, T. and Schienle, M.},
  year = {2021},
  month = nov,
  pages = {2021.11.05.21265810},
  institution = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2021.11.05.21265810},
  abstract = {We report on the second and final part of a pre-registered forecasting study on COVID-19 cases and deaths in Germany and Poland. Fifteen independent research teams provided forecasts at lead times of one through four weeks from January through mid-April 2021. Compared to the first part (October\textendash December 2020), the number of participating teams increased, and a number of teams started providing subnational-level forecasts. The addressed time period is characterized by rather stable non-pharmaceutical interventions in both countries, making short-term predictions more straightforward than in the first part of our study. In both countries, case counts declined initially, before rebounding due to the rise of the B.1.1.7 variant. Deaths declined through most of the study period in Germany while in Poland they increased after a prolonged plateau. Many, though not all, models outperformed a simple baseline model up to four weeks ahead, with ensemble methods showing very good relative performance. Major trend changes in reported cases, however, remained challenging to predict.},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DXDPZ8TC/Bracher et al. - 2021 - National and subnational short-term forecasting of.pdf;/Users/nikos/github-synced/zotero-nikos/storage/Y7YB8TYG/2021.11.05.html}
}

@article{bracherShorttermForecastingCOVID192021,
  title = {Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}} during the Second Wave \textendash{} a Preregistered Study},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, J. and G{\"o}rgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, Nikos I. and Burgard, J. P. and Castro, L. and Fairchild, G. and Fuhrmann, J. and Funk, S. and Gogolewski, K. and Gu, Q. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Li, M. L. and Meinke, J. H. and Michaud, I. J. and Niedzielewski, K. and O{\.z}a{\'n}ski, T. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Zieli{\'n}ski, J. and Zou, D. and Gneiting, T. and Schienle, M.},
  year = {2021},
  month = jan,
  journal = {medRxiv},
  pages = {2020.12.24.20248826},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.12.24.20248826},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}We report insights from ten weeks of collaborative COVID-19 forecasting for Germany and Poland (12 October \textendash{} 19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BHPBLCD9/Bracher et al. - 2021 - Short-term forecasting of COVID-19 in Germany and .pdf;/Users/nikos/github-synced/zotero-nikos/storage/I3ULULUZ/2020.12.24.20248826v2.html}
}

@misc{cramerCOVID19ForecastHub2020,
  title = {{{COVID-19 Forecast Hub}}: 4 {{December}} 2020 Snapshot},
  shorttitle = {{{COVID-19 Forecast Hub}}},
  author = {Cramer, Estee and Nicholas G Reich and Serena Yijin Wang and Jarad Niemi and Abdul Hannan and Katie House and Youyang Gu and Shanghong Xie and Steve Horstman and {aniruddhadiga} and Robert Walraven and {starkari} and Michael Lingzhi Li and Graham Gibson and Lauren Castro and Dean Karlen and Nutcha Wattanachit and {jinghuichen} and {zyt9lsb} and {aagarwal1996} and Spencer Woody and Evan Ray and Frost Tianjian Xu and Hannah Biegel and GuidoEspana and Xinyue X and Johannes Bracher and Elizabeth Lee and {har96} and {leyouz}},
  year = {2020},
  month = dec,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3963371},
  abstract = {This update to the COVID-19 Forecast Hub repository is a snapshot as of 4 December 2020 of the data hosted by and visualized at~https://covid19forecasthub.org/.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/AVWA2UPE/4305938.html}
}

@article{elliottForecastingEconomicsFinance2016,
  title = {Forecasting in {{Economics}} and {{Finance}}},
  author = {Elliott, Graham and Timmermann, Allan},
  year = {2016},
  journal = {Annual Review of Economics},
  volume = {8},
  number = {1},
  pages = {81--110},
  doi = {10.1146/annurev-economics-080315-015346},
  abstract = {Practices used to address economic forecasting problems have undergone substantial changes over recent years. We review how such changes have influenced the ways in which a range of forecasting questions are being addressed. We also discuss the promises and challenges arising from access to big data. Finally, we review empirical evidence and experience accumulated from the use of forecasting methods to a range of economic and financial variables.},
  keywords = {big data,forecast evaluation,forecast models,model instability,model misspecification,parameter estimation,risk},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-economics-080315-015346},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3AXCLA59/Elliott and Timmermann - 2016 - Forecasting in Economics and Finance.pdf}
}

@article{funkAssessingPerformanceRealtime2019,
  title = {Assessing the Performance of Real-Time Epidemic Forecasts: {{A}} Case Study of {{Ebola}} in the {{Western Area}} Region of {{Sierra Leone}}, 2014-15},
  shorttitle = {Assessing the Performance of Real-Time Epidemic Forecasts},
  author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Lowe, Rachel and Eggo, Rosalind M. and Edmunds, W. John},
  year = {2019},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {2},
  pages = {e1006785},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006785},
  abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013\textendash 16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
  langid = {english},
  keywords = {Epidemiology,Forecasting,Infectious disease epidemiology,Infectious diseases,Mathematical models,Probability distribution,Public and occupational health,Sierra Leone},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf;/Users/nikos/github-synced/zotero-nikos/storage/JN28VVKF/article.html}
}

@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUCMSBKN/j.1467-9868.2007.00587.html}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf}
}

@article{gneitingWeatherForecastingEnsemble2005,
  title = {Weather {{Forecasting}} with {{Ensemble Methods}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E.},
  year = {2005},
  month = oct,
  journal = {Science},
  volume = {310},
  number = {5746},
  pages = {248--249},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1115255},
  abstract = {{$<$}p{$>$} Traditional weather forecasting has been built on a foundation of deterministic modeling--start with initial conditions, put them into a supercomputer model, and end up with a prediction about future weather. But as Gneiting and Raftery discuss in their Perspective, a new approach--ensemble forecasting--was introduced in the early 1990s. In this method, up to 100 different computer runs, each with slightly different starting conditions or model assumptions, are combined into a weather forecast. In concert with statistical techniques, ensembles can provide accurate statements about the uncertainty in daily and seasonal forecasting. The challenge now is to improve the modeling, statistical analysis, and visualization technologies for disseminating the ensemble results. {$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {\textcopyright{} 2005 American Association for the Advancement of Science},
  langid = {english},
  pmid = {16224011},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VRJMN77J/Gneiting and Raftery - 2005 - Weather Forecasting with Ensemble Methods.pdf;/Users/nikos/github-synced/zotero-nikos/storage/8Q5UA2FU/248.html}
}

@article{kukkonenReviewOperationalRegionalscale2012,
  title = {A Review of Operational, Regional-Scale, Chemical Weather Forecasting Models in {{Europe}}},
  author = {Kukkonen, J. and Olsson, T. and Schultz, D. M. and Baklanov, A. and Klein, T. and Miranda, A. I. and Monteiro, A. and Hirtl, M. and Tarvainen, V. and Boy, M. and Peuch, V.-H. and Poupkou, A. and Kioutsioukis, I. and Finardi, S. and Sofiev, M. and Sokhi, R. and Lehtinen, K. E. J. and Karatzas, K. and San Jos{\'e}, R. and Astitha, M. and Kallos, G. and Schaap, M. and Reimer, E. and Jakobs, H. and Eben, K.},
  year = {2012},
  month = jan,
  journal = {Atmospheric Chemistry and Physics},
  volume = {12},
  number = {1},
  pages = {1--87},
  publisher = {{Copernicus GmbH}},
  issn = {1680-7316},
  doi = {10.5194/acp-12-1-2012},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Numerical models that combine weather forecasting and atmospheric chemistry are here referred to as chemical weather forecasting models. Eighteen operational chemical weather forecasting models on regional and continental scales in Europe are described and compared in this article. Topics discussed in this article include how weather forecasting and atmospheric chemistry models are integrated into chemical weather forecasting systems, how physical processes are incorporated into the models through parameterization schemes, how the model architecture affects the predicted variables, and how air chemistry and aerosol processes are formulated. In addition, we discuss sensitivity analysis and evaluation of the models, user operational requirements, such as model availability and documentation, and output availability and dissemination. In this manner, this article allows for the evaluation of the relative strengths and weaknesses of the various modelling systems and modelling approaches. Finally, this article highlights the most prominent gaps of knowledge for chemical weather forecasting models and suggests potential priorities for future research directions, for the following selected focus areas: emission inventories, the integration of numerical weather prediction and atmospheric chemical transport models, boundary conditions and nesting of models, data assimilation of the various chemical species, improved understanding and parameterization of physical processes, better evaluation of models against data and the construction of model ensembles.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X3N7D4HE/Kukkonen et al. - 2012 - A review of operational, regional-scale, chemical .pdf;/Users/nikos/github-synced/zotero-nikos/storage/XWR2S6F8/2012.html}
}

@article{mathesonScoringRulesContinuous1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  year = {1976},
  month = jun,
  journal = {Management Science},
  volume = {22},
  number = {10},
  pages = {1087--1096},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf;/Users/nikos/github-synced/zotero-nikos/storage/H5CNZS4U/mnsc.22.10.html}
}

@misc{sherrattPredictivePerformanceMultimodel2022,
  title = {Predictive Performance of Multi-Model Ensemble Forecasts of {{COVID-19}} across {{European}} Nations},
  author = {Sherratt, K. and Gruson, H. and Grah, R. and Johnson, H. and Niehus, R. and Prasse, B. and Sandman, F. and Deuschel, J. and Wolffram, D. and Abbott, S. and Ullrich, A. and Gibson, G. and Ray, El and Reich, Ng and Sheldon, D. and Wang, Y. and Wattanachit, N. and Wang, L. and Trnka, J. and Obozinski, G. and Sun, T. and Thanou, D. and Pottier, L. and Krymova, E. and Barbarossa, Mv and Leith{\"a}user, N. and Mohring, J. and Schneider, J. and Wlazlo, J. and Fuhrmann, J. and Lange, B. and Rodiah, I. and Baccam, P. and Gurung, H. and Stage, S. and Suchoski, B. and Budzinski, J. and Walraven, R. and Villanueva, I. and Tucek, V. and {\v S}m{\'i}d, M. and Zaj{\'i}cek, M. and {\'A}lvarez, C. P{\'e}rez and Reina, B. and Bosse, Ni and Meakin, S. and Loro, P. Alaimo Di and Maruotti, A. and Eclerov{\'a}, V. and Kraus, A. and Kraus, D. and Pribylova, L. and Dimitris, B. and Li, Ml and Saksham, S. and Dehning, J. and Mohr, S. and Priesemann, V. and Redlarski, G. and Bejar, B. and Ardenghi, G. and Parolini, N. and Ziarelli, G. and Bock, W. and Heyder, S. and Hotz, T. and Singh, D. E. and {Guzman-Merino}, M. and Aznarte, Jl and Mori{\~n}a, D. and Alonso, S. and {\'A}lvarez, E. and L{\'o}pez, D. and Prats, C. and Burgard, Jp and Rodloff, A. and Zimmermann, T. and Kuhlmann, A. and Zibert, J. and Pennoni, F. and Divino, F. and Catal{\`a}, M. and Lovison, G. and Giudici, P. and Tarantino, B. and Bartolucci, F. and Lasinio, G. Jona and Mingione, M. and Farcomeni, A. and Srivastava, A. and {Montero-Manso}, P. and Adiga, A. and Hurt, B. and Lewis, B. and Marathe, M. and Porebski, P. and Venkatramanan, S. and Bartczuk, R. and Dreger, F. and Gambin, A. and Gogolewski, K. and {Gruziel-Slomka}, M. and Krupa, B. and Moszynski, A. and Niedzielewski, K. and Nowosielski, J. and Radwan, M. and Rakowski, F. and Semeniuk, M. and Szczurek, E. and Zielinski, J. and Kisielewski, J. and Pabjan, B. and Holger, K. and Kheifetz, Y. and Scholz, M. and Bodych, M. and Filinski, M. and Idzikowski, R. and Krueger, T. and Ozanski, T. and Bracher, J. and Funk, S.},
  year = {2022},
  month = jun,
  pages = {2022.06.16.22276024},
  publisher = {{medRxiv}},
  doi = {10.1101/2022.06.16.22276024},
  abstract = {Background Short-term forecasts of infectious disease burden can contribute to situational awareness and aid capacity planning. Based on best practice in other fields and recent insights in infectious disease epidemiology, one can maximise the predictive performance of such forecasts if multiple models are combined into an ensemble. Here we report on the performance of ensembles in predicting COVID-19 cases and deaths across Europe between 08 March 2021 and 07 March 2022. Methods We used open-source tools to develop a public European COVID-19 Forecast Hub. We invited groups globally to contribute weekly forecasts for COVID-19 cases and deaths reported from a standardised source over the next one to four weeks. Teams submitted forecasts from March 2021 using standardised quantiles of the predictive distribution. Each week we created an ensemble forecast, where each predictive quantile was calculated as the equally-weighted average (initially the mean and then from 26th July the median) of all individual models' predictive quantiles. We measured the performance of each model using the relative Weighted Interval Score (WIS), comparing models' forecast accuracy relative to all other models. We retrospectively explored alternative methods for ensemble forecasts, including weighted averages based on models' past predictive performance. Results Over 52 weeks we collected and combined up to 28 forecast models for 32 countries. We found a weekly ensemble had a consistently strong performance across countries over time. Across all horizons and locations, the ensemble performed better on relative WIS than 84\% of participating models' forecasts of incident cases (with a total N=862), and 92\% of participating models' forecasts of deaths (N=746). Across a one to four week time horizon, ensemble performance declined with longer forecast periods when forecasting cases, but remained stable over four weeks for incident death forecasts. In every forecast across 32 countries, the ensemble outperformed most contributing models when forecasting either cases or deaths, frequently outperforming all of its individual component models. Among several choices of ensemble methods we found that the most influential and best choice was to use a median average of models instead of using the mean, regardless of methods of weighting component forecast models. Conclusions Our results support the use of combining forecasts from individual models into an ensemble in order to improve predictive performance across epidemiological targets and populations during infectious disease epidemics. Our findings further suggest that median ensemble methods yield better predictive performance more than ones based on means. Our findings also highlight that forecast consumers should place more weight on incident death forecasts than incident case forecasts at forecast horizons greater than two weeks. Code and data availability All data and code are publicly available on Github: covid19-forecast-hub-europe/euro-hub-ensemble.},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BTXYEE73/Sherratt et al. - 2022 - Predictive performance of multi-model ensemble for.pdf;/Users/nikos/github-synced/zotero-nikos/storage/6EVU3A8F/2022.06.16.html}
}

@article{timmermannForecastingMethodsFinance2018,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  doi = {10.1146/annurev-financial-110217-022713},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-financial-110217-022713}
}

@article{timmermannForecastingMethodsFinance2018a,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  month = nov,
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  issn = {1941-1367, 1941-1375},
  doi = {10.1146/annurev-financial-110217-022713},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9FHI5V4F/Timmermann - 2018 - Forecasting Methods in Finance.pdf}
}

@article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {Winkler, R. L. and Mu{\~n}oz, Javier and Cervera, Jos{\'e} L. and Bernardo, Jos{\'e} M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and {R{\'i}os-Insua}, David},
  year = {1996},
  month = jun,
  journal = {Test},
  volume = {5},
  number = {1},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for ``good'' probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of ``goodness'' of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf}
}


