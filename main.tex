\documentclass{article}
\usepackage[]{graphicx}
\usepackage[]{xcolor}
\usepackage{alltt}
\usepackage[left=2.3cm,right=2.8cm, top = 2.2cm, bottom = 3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\PassOptionsToPackage{hyphens}{url}
\usepackage{url} 
\usepackage[disable]{todonotes}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage[colorlinks=false]{hyperref} 
\urlstyle{same}
\usepackage{lineno}
\linenumbers


% to handle authorship footnotes as numbers:
\makeatletter
\let\@fnsymbol\@arabic
\makeatother

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand{\changed}[1]{#1}


\begin{document}


\title{To log or not to log}
  \author{Anonymous Alpaca\thanks{All Alpaca friends} $^{,}$\thanks{The Zoo} $^{ , *}$}

\maketitle

\tableofcontents

\begin{abstract}
In the abstract, this article is a very good one. We did a very good job of the research and got all the best results. People told us we were mad and we said no we are effective. Now gather around and listen to our thoughts you will be, shocked, scared, amazed, and astounded.
\end{abstract}

\bigskip

{\footnotesize $^*$ Correspondence to: Anonymous Alpaca (\url{anonymous@alpaca.com}))}



\newpage


% ===========================================================
\section{Introduction}

Forecasts play an important role in decision-making in epidemiology and public health [Citations], as well as other areas as diverse as for example economics [Citations] or agriculture [Citations]. Epidemiological modelling has been an important tool to inform public policy throughout the COVID-19 pandemic and scenario modelling, as well as forecasts of cases, hospitalisations and deaths from COVID-19 have received widespread attention. Beginning in 2020, forecasts from different research institutions have been systematically collected, aggregated and evaluated by COVID-19 Forecast hubs in the US [Citation], Germany and Poland, and Europe [Citation]. 

We usually understand epidemiological processes to be exponential in nature and therefore most commonly describe and model them in terms of concepts like growth rates [CITATION?] or the effective reproduction number (the average number of secondary cases produced by each infected person) [CITATION]. Consequently, errors from such models will generally be dominated by multiplicative process errors. Under the assumption that these models are correctly capturing the underlying inherent transmission dynamics, it would make sense to focus evaluation on these multiplicative errors. In the past, however, evaluation of forecasts submitted to the Forecast Hubs was mostly centred around metrics like the weighted interval score (WIS), a proper scoring rule [CITATION] which, applied to raw forecasts, captures additive, rather than multiplicative errors. This makes it harder to diagnose and quantify model mis-specification, but also makes it more difficult to deal with the potential non-stationarity of epidemiological processes, or to compare performance across targets that are on very different orders of magnitudes (like for example cases and hospitalisations). In this paper, we propose to transform forecasts and observed values prior to evaluation using the WIS in a way that shifts the focus of the evaluation towards relative, rather than absolute errors. We illustrate the effect of this transformation using forecasts submitted to the European COVID-19 Forecast Hub. 

\paragraph{Forecast Hub format and WIS}
Forecasts submitted to the European (as well as other) COVID-19 Forecast Hubs are probabilistic in nature [Citation e.g. Held et al.], meaning that forecasters provide a full predictive distribution which quantifies their uncertainty. These predictive distributions where reported in the form of a set of 23 quantiles (11 symmetric prediction intervals as well as a median prediction). The quality of the forecasts was mainly assessed based on two grounds: first, by how well they were calibrated (judged primarily by whether the 50\% and 90\% central prediction intervals on average really covered 50\% and 90\% of observed values, respectively) and second, by how well they scored in terms of the weighted interval score [CITATION bracher]. The WIS is a strictly proper scoring rule, meaning that it incentivises any forecaster to state their true best belief and in expectation gives the best score to the forecaster who reports a predictive distribution that is equal to the data-generating distribution. 

WIS values are always larger or equal than zero and lower values imply better performance. The WIS can be decomposed into a dispersion component and penalties for over- and under-prediction. For a single prediction interval, the interval score is computed as 
\begin{align}
 IS_\alpha(F,y) &= (u-l) + \frac{2}{\alpha} \cdot (l-y) \cdot 1(y \leq l) + \frac{2}{\alpha} \cdot (y-u) \cdot 1(y \geq u) \\
 &= \text{dispersion} + \text{underprediction} + \text{overprediction},    
\end{align}

where $1()$ is the indicator function, $y$ is the observed value, and $l$ and $u$ are the $\frac{\alpha}{2}$ and $1 - \frac{\alpha}{2}$ quantiles of the predictive distribution $F$, i.e. the lower and upper bound of a single central prediction interval. For a single forecast interval, the interval score is the width of the prediction interval, if it covers the observed value, and the absolute error between the forecast and the lower (or upper, respectively) interval boundary. For a set of $K$ prediction intervals and the median $m$, the WIS is computed as a weighted sum, 
\begin{equation}
\text{WIS} = \frac{1}{K + 0.5} \cdot \left(w_0 \cdot |y - m| + \sum_{k = 1}^{K} w_k \cdot IS_{\alpha}(F, y)\right),    
\end{equation} 
where $w_k$ is a weight for every interval. Usually, $w_k = \frac{\alpha_k}{2}$ and $w_0 = 0.5$

The WIS is closely related to the continuous ranked probability score (CRPS) [Cite Gneiting 2007], which itself can be understood as a generalisation of the absolute error to probabilistic forecasts. For an increasing set of equally-spaced prediction intervals the WIS converges to the CRPS and shares many of its properties. More information on the CRPS is provided in section \ref{crps} in the SI. In this paper we focus on the WIS due to its simplicity and widespread use by the COVID-19 Forecast Hubs, but arguments apply analogously to the CRPS. Another proper scoring rule commonly used is the log score [CITATION], which we will not discuss further in this paper.  

%Simplified to the case of a point prediction, we could express the future observed value $y_t$ we as the point prediction $\hat{y}_{t+1}$ plus an additive error $\varepsilon_{t+1}$:
%
%\begin{equation}
%    y_{t+1} = \hat{y}_{t+1} + \varepsilon_{t+1}. 
%Â¸\end{equation} 
%Conceptually, the WIS and the CRPS evaluate the forecast based on this additive error $\varepsilon_{t+1}$. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Transforming forecasts to shift the focus of the evaluation}

Applied to the 'raw' forecasts, the WIS measures the absolute distance between the predictive distribution and the forecast. We can, however, shift the focus of the evaluation by transforming the forecasts. 
Any forecast about the absolute value of a forecast target in the future implicitly also contains a statement about the growth rate of the underlying process (assuming that we have accurate knowledge of the target value today). The growth rate is defined as 
%
\begin{equation}
    g_{t, t+1} = \frac{y_{t+1} - y_t}{y_t},
\end{equation}
%
where $y_{t+1}$ is the forecast target (e.g. reported cases of COVID-19) in the future, $y_t$ is the last known observation and $g_{t, t+1}$ is the growth rate between now ($t$) and $t+1$. 
Using the growth rate, we can express the value of the forecast target in the future as such: 
%
\begin{equation}
y_{t+1} = (1 + g_{t, t+1}) \cdot y_t.
\end{equation}
%

We discuss two different transformations in order to shift the focus towards evaluating the forecaster based on how well they predicted the growth relative to the present value, rather than how well they predicted the absolute value that was observed. The first one is to divide all observed values, and the forecasts by the last observed value. The second one is to take the logarithm of all forecasts and observed values. The computed score is still proper, as for a single forecast the order between different forecasters is not affected by the monotone transformation (citation).

\paragraph{Scoring the growth rate directly}
By dividing every observed value and every forecast by the last observed value prior to evaluation, we replace forecasting $y_{t+1}$ with forecasting  $y_{t+1} / y_t = 1 + g_{t, t+1}$. Just as we were previously penalised an additive error with respect the absolute value, we are now penalising an absolute error with respect to the growth rate. 


We can illustrate what's happening by thinking of an analogous point prediction $\hat{g_{t, t+1}}$ for the growth rate. We could then write
\begin{equation}
    1 + g_{t, t+1} = 1 + \hat{g}_{t, t+1} + \varepsilon^*_{t+1}. 
\end{equation}
%
The WIS and CRPS would evaluate the forecast based on the error $\varepsilon^*_{t+1}$. This translates to an absolute error on the actual forecast target that is multiplicatively connected to the last observed value: 
\begin{align}
    y_{t+1} &= (1 + \hat{g}_{t, t+1} + \varepsilon^*_{t+1}) \cdot y_t \\
            &= (1 + \hat{g}_{t, t+1}) \cdot y_t + \varepsilon^*_{t+1} \cdot y_t \\
            &= \hat{y}_{t+1} + \varepsilon^*_{t+1} \cdot y_t.
\end{align}

One disadvantage of this is that we can't really do anything when an observation is zero. 
%is this helpful?%
%Maybe add some math on how we actually transform the distribution. E.g. divide every quantile or sample by the last observed value, divide the distribution?%

\paragraph{Taking the logarithm of forecasts}
Taking the logarithm of both the forecasts and the observed values means that we are now scoring an additive error on $\log y_{t_+1}$ rather than on $y_{t+1}$ (see a detailed derivation in section \ref{wis-log-derivation} in the SI). 

Using again the analogy of a point forecast it is easy to see that this corresponds to evaluating a multiplicative error on the original $y_{t+1}$:
%
\begin{align}
\log y_{t+1} &= \log \hat{y}_{t+1} + \varepsilon_{t+1} \Leftrightarrow \\    
y_{t+1} &= (1 + \hat{y}_{t+1}) \cdot \exp{\varepsilon_{t+1}}.    
\end{align}
%
Computing a score based on the logarithm of the forecast and the observation is approximately equivalent to scoring a forecast of a multiplicative growth rate of the forecast target. 


To get an intuition, using again the analogy of point forecasts, note that we can decompose the error into a known and an unknown part: 
%
\begin{align}
\log y_{t+1} &= \log (1 + \hat{g}_{t, t+1}) + \log y_t + \varepsilon_{t+1} \Leftrightarrow \\
\log y_{t+1} &= \log (1 + \hat{g}_{t, t+1}) + \log y_t - \log y_t + \varepsilon'_{t+1} \Leftrightarrow \\    
\log y_{t+1} &= \log (1 + \hat{g}_{t, t+1}) + \varepsilon'_{t+1}.
\end{align}
%
% NOT SURE THIS ARGUMENT WORKS. 
%
What affects the score is only the unknown part, as shifting both the forecast and the observation by a known amount does not make a difference for evaluating the distance between forecast and observation. The WIS and CRPS therefore provide a measurement of an additive error on the forecast of $ \log (1 + \hat{g}_{t, t+1})$. For small values of $g$, we can make the following approximation
\begin{equation}
    \log (1+ g_{t, t+1}) \approx g_{t, t+1}.
\end{equation}
%

Conveniently, the decomposition of the WIS into dispersion, overprediction and underprediction is preserved for both transformations. The interpretation of the components has now changed in that they now (approximately) represent uncertainty, over-prediction, and under-prediction with the growth rate and so are independent of current incidence. 

\paragraph{First transform, then score, rather than the other way round}
Note that simply taking the log of a score computed on the natural scale, i.e. computing $\log S(x, y)$ results in an improper score and is therefore not advised. We illustrate this point in Figure \ref{fig:log-improper} in the SI. 


\paragraph{How scores change}
For a single forecast, the ranking between forecasters would be preserved and the scores remain proper [Proof?]. Differences in the ranking between forecasters only appear when averaging over multiple forecasts, as deviations from the perfect forecast get penalised differently depending on whether forecasts are transformed or not. 

The log transformation therefore changes which types of errors (additive or multiplicative) are penalised. The WIS and CRPS computed on the natural scale increase linearly with increasing absolute errors (see Figure \ref{fig:change-in-scores}A, regardless of whether the error is large in relative terms. Predicting 101,000 cases instead of the true 100,000 will be treated equally to predicting 2,000 hospitalisations, rather than the 1,000 observed. The WIS and CRPS based on log-transformed values however, penalise relative errors and increase linearly with an increase in relative errors (see Figure \ref{fig:change-in-scores}D). Forecasting 101,000 rather than 100,000 cases will be treated equally to forecasting 2,020, rather than 2,000 hospitalisations. 

\paragraph{What happens to the WIS / CRPS when you log it?}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/SIM-effect-log-score.png}
    \caption{Weighted interval score for a forecast distribution that is N(1, 0.2) and different observed values. Top: difference in absolute terms. Bottom: difference in relative terms. Interesting panels are top left and bottom right.}. 
    \label{fig:log-crps-viz}
\end{figure}




\section{Appropriate scales for epidemiological processes}

THE NEXT PARAGRAPH COULD BE MOVED TO THE INTRODUCTION?

In this section we explore the rationale for and implications of scoring transformed forecasts in an epidemiological context in greater depth. Forecast evaluation serves several purposes. It provides feedback for forecasters and researchers necessary to improve their models, and it helps decision makers with distinguishing good from bad predictions and choosing forecasts that inform future decisions. 

Generally, the way forecasts are evaluated should correspond to the error structure of the model and the type of errors models will usually make. In epidemiology, the spread of infectious diseases is usually modelled as an exponential process. COMPARTMENT MODELS. RENEWAL EQUATION. The number of infections usually grows with some exponential rate etc. Error from these models will then in general be dominated by multiplicative process errors. Under the assumption that these models are correctly capturing transmission dynamics inherent to the system it therefore makes sense to focus evaluation on these multiplicative errors. Often, if the model is performing badly in multiplicative terms, this means that there is a flaw with its assumptions that needs to be resolved, regardless of performance in absolute terms. When selecting a model for decision-making, similar considerations apply. When a model is structured in a multiplicative way, we should base our trust in it on past relative performance, rather than absolute performance. Though ultimately this decision is dependent on the users of the forecast who may prefer models that perform better on an absolute scale for a range of reasons (for example not over-estimating the peak size of an epidemic). 

SHOULD INSTEAD TALK ABOUT WHAT THE DIFFERNCE IS BETWEEN LOG AND DIVIDING? 

\paragraph{What happens when you average things}
In order to compare models, we usually need to compare aggregate performance. In many epidemiological settings, whether we decide to evaluate relative or absolute performance has implications for how we weigh performance of models across different dimensions when we aggregate and compare scores. For example, different forecast targets like reported case numbers, hospitalisations and deaths have very different orders of magnitude and scores (even for an ideal forecaster) usually also differ by several orders of magnitude. If we aggregated scores on the natural scale across e.g. cases and deaths, in order to rank models by overall score, an equal weight on absolute performance would implicitly result in a much higher weight on performance on cases. On the log scale, we would put equal weight on the relative performance on cases and on deaths, making performance usually more easily comparable (as scores of an ideal forecaster would not differ as much as on the natural scale). 
IS THAT TRUE? SHOULD PROBABLY PROVE THIS POINT BY SIMULATION. 

While it is intuitive that performance on cases is not orders of magnitude more important than performance on deaths, the question becomes trickier if we compare across locations or across time for the same forecast target. Indeed, predictive performance of a model may be more important in locations or at times when incidence values are high, as argued by \citep{Bracher}. Whether or not we should think this way depends on certain characteristics of the models. 

Let us assume that we care in particular about situations where incidence numbers are high. It could be useful to give a higher weight to these situations (by scoring on the natural scale), if we believe that certain models may indeed be inherently better in such situations where incidence numbers are high. For example, models may be better around epidemic peaks when numbers are highest, or they may be especially good at forecasting large-scale population dynamics. %(whatever that may be...) 
Then, scoring on the natural scale will help you identify identify the models that tend to be most situations in the situations we most care about. For example, when looking at one forecast target across time, it may be reasonable to assume that the model which performed best around the peak of one wave of COVID-19 may also be the one to take most seriously in the second wave of COVID-19. If, on the other hand, we believe that good models should do consistently well and that situations with high incidences do not provide orders of magnitude more information about which models perform well or badly in the circumstances we most care about, then scoring on the log scale may be more appropriate. For example, it may be reasonable not to think that predictive performance in a country like Germany would have a hundred times more importance than performance in a country like Luxembourg when determining the best model for future decision making. Similarly, we may not necessarily believe that predictive performance during the January 2022 wave of COVID-19 should carry three times as much weight than predictive performance during the January 2021 wave, just because numbers where three times as high. 
% Maybe make a plot with one country and scores for that one country in different waves. We could then argue that within one wave, performance of the peak is meaningful, but across different waves (with different numbers) it is unclear, whether that difference conveys meaningful information. 

When scoring forecasts on a log scale, one may fall in the opposite trap of giving to much weight to situations where incidences are low, as small errors (e.g. predicting 9, rather than 3 deaths) may have a much larger effect in relative terms than in absolute terms if the quantity to forecast is very small. How much weight is given to smaller or larger forecast quantities depends on the exact relationship between the mean and the variance of the quantity of interest. To investigate this we sampled forecasts from different negative binomial distributions. The negative binomial distribution has mean $\mu$ and variance $\sigma^2 = \mu + \mu ^2 / \theta$ and for $\lim_{\theta \to \infty}$ converges to the Poisson distribution. For large values of $\theta$, resulting in a variance approximately equal to the mean, forecasts for lower quantities on average received higher scores on the log scale (see Figure \ref{fig:SIM-wis-state-size-mean}. For $\theta = 1$ (and correspondingly, $\sigma^2 = \mu + \mu^2$, we found that scores on the log scale remained approximately constant regardless of the size of the quantity to forecast. For $\theta = 0.1$ (and $\sigma^2 = \mu + 10 \cdot \mu^2$), scores on the log scale increased with the quantity to forecast. When scored on the natural scale, a higher quantity to forecast always lead to higher scores regardless of the chosen distribution (as long as the variance grows together with the mean). 

%MAYBE A PARAGRAPH ABOUT OUTLIERS?
%COULD ALSO MAKE A NICE PLOT ABOUT OUTLIERS, where we look at the effect of outliers. but maybe that is already included in the plot in Figure 1. 
% could make an analysis re outliers: how do scores change if we remove the 1 or 2 worst forecasts? The one or two best forecasts?

%THIS COULD INCLUDE JOHANNES EXAMPLE

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/SIM-mean-sd-state-size.png}
    \caption{Simulation of the effect of population size with ideal forecasts of a negative-binomially-distributed variable. For each simulated state, we drew 1,000 observations from a negative binomial distribution with $\mu = 100 \cdot \text{state size}$ and with values of $\theta$ equal to 0.1, 1, and 1 billion. The variance of the negative binomial is given as $\sigma^2 = \mu + \mu^2 / \theta$, meaning that for large theta the negative binomial distribution is equal to the poisson distribution. For these simulated values we computed the WIS for an ideal forecast (i.e. the predictive distribution was negative binomial with $\mu$ and $\theta$ equal to the true $\mu$ and $\theta$ for every state). Left: Mean WIS depending on state size, right: Mean WIS depending on state sizes when scored on a log scale. Plots for the standard deviation, rather than the mean of WIS values look are given in Figure \ref{fig:SIM-wis-state-size-sd} in the SI.}. 
    \label{fig:SIM-wis-state-size-mean}
\end{figure}


%Other transformations: 
%Box-Cox, square root 

%Score different things from the European / US Forecast Hub and plot the relationship between mean and variance. Plot log-scale scores for different things and see which of these influence average scores most \\
% What happens to the decomposition of the WIS if we log? 


%COMPARISON TO SCORING THE ACTUAL GROWTH RATE - just harder to do and achieves the same thing? No objections if someone wants to? Might actually be better if there are negative values involved? 

%What about other things like scoring the square root of the forecast?


%\begin{itemize}
%    \item discuss how equations change for CRPS
%    \item Optional: Discussion of parallels to point forecasts and the point that you're still incentivised to report the median\\
%    \item discuss that even if both scoring rules are proper, they still penalise different things %differently. How does that change incentives for the forecaster? 
%\end{itemize}


\paragraph{Toy example: simulate an epidemic and apply 3 different models} Compare scores for these three models on the natural scale and the log scale


\section{Empirical example: the European Forecast Hub}

\paragraph{Introduction to the Forecast Hub} As an empirical example for evaluating forecasts on the natural and on the log scale we use forecasts from the European Forecast Hub \citep{europeancovid-19forecasthubEuropeanCovid19Forecast2021}. Every week the European Forecast Hub collates and aggregates forecasts for different COVID-19 related targets from teams around the world. Forecasts are made one to four weeks ahead into the future and follow a quantile-based format with a set of 22 quantiles plus the median ($0.01, 0.025, 0.05, ..., 0.5, ... 0.95, 0.975, 0.99$). The forecasts for the purpose of this illustrations are two-week-ahead forecasts made between XXX and XXX for reported cases and deaths from COVID-19 in XXX countries. 

\paragraph{Describing features of the data}
Across all time points and locations, the mean number of cases (deaths) observed was 22811 (368) with an overall standard deviation of 47135 (831). The number of observed value does not only differ by forecast target, but also shows considerable heterogeneity by location (see Figure \ref{fig:HUB-mean-locations}. Across the different locations, the variance in observed values is roughly equal to the square of the mean, suggesting that, for a given location, both cases and deaths are not poisson-distributed, but rather exhibit over-dispersion. 
MAYBE FIT A NEGATIVE BINOMIAL TO THESE OBSERVATIONS

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/HUB-mean-obs-location.png}
    \caption{CAPTION}
    \label{fig:HUB-mean-locations}
\end{figure}

\paragraph{Features of the scores}
Across all time points and locations, the mean number weighted interval score on the natural scale for cases (deaths) was 8855 (59.3) with an overall standard deviation of 51274 (156). On the log scale, scores were much closer to each other (see Figure \ref{fig:HUB-average-scores}), with a mean wis for cases (deaths) of 0.507 (0.362) with an overall standard deviation of 0.745 (0.407). 
% NEED A TABLE WITH SCORES for the Appendix
On the natural scale, scores vary by several orders of magnitudes across locations (see \ref{fig:HUB-scores-location}), whereas variation across locations is much smaller for scores on the log scale and wis values are more evenly distributed. The ordering of scores across locations is not preserved when going from the natural to the log scale. 
% Rather, there seems to be a slight inverse relationship that locations which previously had low scores now tend to receive higher scores. 
The correlation between scores on the log and natural scale for different locations is XX for cases and XX for deaths. 
%THERE SEEMS TO BE A TENDENCY FOR SMALLER 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/HUB-average-scores.png}
    \caption{Distribution of interval scores for two week ahead forecasts of COVID-19 cases and deaths evaluated on the natural scale (left) and on the log scale (right). }
    \label{fig:HUB-average-scores}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/HUB-scores-locations.png}
    \caption{Weighted interval scores across different locations for two week ahead forecasts of COVID-19 cases and deaths evaluated on the natural scale (left) and on the log scale (right). }
    \label{fig:HUB-scores-location}
\end{figure}

When evaluated on the natural scale, there is  strong relationship between the mean of the weighted interval score and the total number of observed cases (correlation: 0.913) or deaths (cor: 0.849) in a location (see Figure \ref{fig:HUB-mean-scores-total-loglog} and Figure \ref{fig:HUB-mean-scores-total} in the SI). The relationship is less pronounced for scores on the log scale (cor: -0.019 for cases, -0.395 for deaths) and negative, meaning that locations with fewer observed cases or deaths tend to receive higher scores. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/HUB-mean-scores-vs-total-log-log.png}
    \caption{Plot with Weighted interval scores against the mean number of observed cases or deaths.}
    \label{fig:HUB-mean-scores-total-loglog}
\end{figure}

Both on the natural scale as well as on the log scale, scores increase considerably with increasing forecast horizon (see Figure \ref{fig:HUB-scores-horizon}). The increase is more pronounced for cases than for deaths and arguably represents an increase in the relative difficulty to forecast values further into the future. A similar increase both on the natural and the log scale indicates that we are able to observe the increase in difficulty regardless of how we evaluate the scores. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/HUB-scores-over-horizon.png}
    \caption{Weighted interval scores across forecast horizons}
    \label{fig:HUB-scores-horizon}
\end{figure}



\section{Other ideas}
\begin{itemize}
    \item Compare different predictive distributions and the score as a function of an outcome.
    \item ...
\end{itemize}




\section{Discussion}

\paragraph{summary of what we did}

\paragraph{context, implications, limitations}

SOME DIFFERENCES BETWEEN SCORES ARE MEANINGFUL. E.G. WE WOULDN'T WANT SCORES FOR CASES AND DEATHS TO BE COMPLETELY EQUAL, BECAUSE DEATHS ARE LIKELY EASIER TO FORECAST

\paragraph{outlook, future work}


 




\newpage

\appendix
\section{Supplementary information}


\subsection{Additional information on the CRPS} \label{crps}

The CRPS measures the 'distance' of the predictive distribution to the observed data-generating distribution as 

\begin{equation}
    \text{CRPS}(F, y) = \int_{-\infty}^\infty \left( F(x) - 1(x \geq y) \right)^2 dx,
\end{equation}

where y is the true observed value and F the CDF of predictive distribution. Often the following alternative representation is used
\begin{equation}
    \text{CRPS}(F, y) = \frac{1}{2} \mathbb{E}_{F} |X - X'| - \mathbb{E}_P |X - y|,
\end{equation}
  
where $X$ and $X'$ are independent realisations from the predictive distributions $F$ with finite first moment and $y$ is the true value. In this representation we can simply replace $X$ and $X'$ by samples sum over all possible combinations to obtain the CRPS.  


\subsection{Changes in the WIS for forecasts on the log scale} \label{wis-log-derivation}

This connection with the underlying epidemic growth rate is an attractive property in and of itself as it is the conceptual target many forecasters and forecast consumers make use of. 

When we score a forecast for $y_{t+1}$ on the log scale, the interval score for a single prediction interval changes as follows
%
\begin{align}
    \text{IS}_\alpha(\log F, \log y) = &(\log u - \log l) \\ 
    &+ \frac{2}{\alpha} \cdot (\log l - \log y) \cdot 1(\log y \leq \log l) \\
    &+ \frac{2}{\alpha} \cdot (\log y - \log u) \cdot 1(\log y \geq \log u).
\end{align}
%
Note that the values of the indicator functions stay the same as the logarithm is a monotonic transformation and the inequality is preserved. 
On the natural scale, $u$, $l$ are the lower and upper bounds of a central prediction interval for a forecast of $y_{t+1} = (1 + g_{t, t+1}) \cdot y_t$. 
We can rewrite these as 
\begin{equation}
   l = l^* \cdot y_t 
   \;\; \text{and} \;\; 
   u = u^* \cdot y_t,  
\end{equation}
where $u^*$ and $l^*$ are the lower and upper bounds of a prediction interval for $(1 + g_{t, t+1})$. Consequently, 
\begin{equation}
  \log l = \log l^* + \log y_t
  \;\; \text{and} \;\;
  \log u = \log u^* + \log y_t. 
\end{equation}
%
We can then rewrite the score for a forecast of $\log y_{t+1} = \log (1 + g_{t,t+1}) + \log y_t$ as
%
\begin{equation}
\begin{aligned}
IS_\alpha&(\log F_{t+1}, \log y_{t+a}) = \\
&((\log u^* + \log y_t) - (\log l^* + \log y_t)) \\
&+ \frac{2}{\alpha} \cdot ((\log l^* + \log y_t) - (\log (1 + g_{t, t+1}) + \log y_t)) 
     \cdot 1((\log (1 + g_{t, t+1}) + \log y_t) \leq (\log l^* + \log y_t) \\
&+ \frac{2}{\alpha} \cdot ((\log (1 + g_{t, t+1}) + \log y_t) - (\log u^* + \log y_t)) 
      \cdot 1((\log (1 + g_{t, t+1}) + \log y_t) \geq (\log u^* + \log y_t),
\end{aligned}
\end{equation}
%
which simplifies to 
%
\begin{equation}
\begin{aligned}
\label{eqn:is-log}
\text{IS}_\alpha(\log F_{t+1}, \log y_{t+a}) = &(\log u^* - \log l^*) \\
&+ \frac{2}{\alpha} \cdot (\log l^* - \log (1 + g_{t, t+1}) 
     \cdot 1(\log (1 + g_{t, t+1}) \leq \log l^*) \\
&+ \frac{2}{\alpha} \cdot (\log (1 + g_{t, t+1}) - \log u^*) 
      \cdot 1(\log (1 + g_{t, t+1}) \geq \log u^*).
\end{aligned}
\end{equation}

The left-hand side of equation \ref{eqn:is-log} was left unchanged. From the right-hand side we can see that for a single prediction interval, scoring the log of a forecast ($\log F_{t+1}$) against the log observed value ($\log y_{t+1}$) is exactly equivalent to evaluating a forecast for $\log (1 + g_{t, t+1})$. With $\log (1 + g_{t, t+1} \approx g_{t, t+1}$, this is approximately a forecast for the growth rate for small values of $g_{t, t+1}$. 


\subsection{Additional figures}




\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/placeholder-image.png}
    \caption{Illustration of the fact that taking the log of WIS or CRPS results in an improper score}
    \label{fig:log-improper}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/HUB-mean-scores-vs-total.png}
    \caption{Plot with Weighted interval scores against the mean number of observed cases or deaths.}
    \label{fig:HUB-mean-scores-total}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{output/figures/SIM-sd-state-size.png}
    \caption{Simulation of the effect of population size with ideal forecasts of a negative-binomially-distributed variable. For each simulated state, we drew 1,000 observations from a negative binomial distribution with $\mu = 100 \cdot \text{state size}$ and with values of $\theta$ equal to 0.1, 1, and 1 billion. The variance of the negative binomial is given as $\sigma^2 = \mu + \mu^2 / \theta$, meaning that for large theta the negative binomial distribution is equal to the poisson distribution. For these simulated values we computed the WIS for an ideal forecast (i.e. the predictive distribution was negative binomial with $\mu$ and $\theta$ equal to the true $\mu$ and $\theta$ for every state). Left: Mean WIS depending on state size, right: Mean WIS depending on state sizes when scored on a log scale. Plots for the standard deviation, rather than the mean of WIS values look are given in Figure \ref{fig:SIM-wis-state-size-sd} in the SI.}. 
    \label{fig:SIM-wis-state-size-mean}
\end{figure}



\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Idea parking lot 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph{Problems with the WIS and CRPS in an epidemiological setting}
% Their relation to the absolute error means that both the WIS and the CRPS usually scale with the prediction target. Forecasts of COVID-19 cases, for example typically have higher scores than forecasts of hospitalisations of death. Similarly, when looking at performance across different locations or over time, average scores will be dominated by locations and times with high incidences. Outliers also can have a disproportionate effect on aggregate scores. One can argue that this is meaningful and that we should care most about places and periods when incidence is high (maybe cite Bracher et al.). However, this may not always be true and is clearly not desirable when comparing performance of a model on different prediction targets: case numbers are not necessarily more important than hospitalisations, just because observed values tend be an order of magnitude higher. This makes forecasts often hard or impossible to compare. 


% not sure this plot is really meaningful
%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.9\textwidth]{output/figures/HUB-sd-vs-mean-scores.png}
%    \caption{CAPTION}
%    \label{fig:HUB-mean-sd-scores}
%\end{figure}

%\begin{itemize}
%    \item idea of scores on logged data: interpretation as a measure of relative improvement, just as when you log the absolute error
%\end{itemize}

%\paragraph{What's there in the literature}

%\paragraph{Current problems / questions about scoring an epidemiological setting}
%probably narrow down to the ones we really want to discuss%
%There are limitations with what we can do with scores on a natural scale and open questions whether we can do these things on a log-scale and also whether a log-scale might be inherently more appropriate
%\begin{itemize}
    %\item Can't really model score on a natural scale as errors are heavily skewed. Is there a way to model scores to get insight about different factors that systematically influence scores
 %   \item make a plot somewhere that shows the distribution of scores
  %  \item Maybe epidemiological forecasts are inherently better suited to be scored on an absolute scale due to the multiplicative nature of processes (related to over-prediction > under-prediction)? Does the log-scale imply we are scoring the growth rate? 
   % \item unclear what trade-offs of logging / not logging are
    %\item what score (log, not log) matches closest what our intuition (or policymakers) think is good? 
    %\item are we better at forecasting deaths than cases? --> relative measure would help
    %\item There exists confusion about what can be done to a score in general (e.g. people want to take the median, which they shouldn't) %maybe don't add as an extra point
%\end{itemize}


%\paragraph{Over- and under-prediction}
%If we want to keep this in, we could: 
%\begin{itemize}
%    \item Check whether there is actually a problem with over-prediction and under-prediction in the Hub. This could be the case because we are most interested in certain scenarios in which this might arise. 
%    \item Discuss this in light of Johannes' analysis of how the decomposition of WIS values differs if the data are skewed
%    \item compare this to the PIT-value-like relative bias scores we used for the German / Polish paper, which capture a relative tendency to over- or under-predict, rather than absolute penalties. 
%\end{itemize}


%\section{Modelling scores}
%\paragraph{Motivation} Alternative is to use summary measures or pairwise comparison, which becomes cumbersome for many dimensions. 

%\paragraph{caveats, practical limitations}
%\begin{itemize}
%    \item What kind of transformations can we do? Propose random effect models
%    \item is there anything we can't do? 
%    \item How does logging change the error distribution? On the natural scale, you have huge outliers. What's the appropriate error distribution to use? confidence intervals. Maybe we can only say something about the effects
%\end{itemize}


%\paragraph{Optional: Application to data from the Euro-Hub}
%Also: what, empirically is the distribution of errors? 